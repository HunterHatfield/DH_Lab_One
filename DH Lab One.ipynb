{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Humanities Lab One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to R and Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to your lab's Jupyter Notebook!\n",
    "\n",
    "A notebook has two primary things. First, it has regular text like this. It's called \"Markdown\" because that's a text format. What's important for us is that this text lets me explain things in text. Secondly, the notebook can include actual code that runs and gives results. The cell (area) below has some of that. To run code in a Jupyter notebook, you have a few options.\n",
    "\n",
    "One: Press the little play button to the top left of the code. Below, it is right next to \"#This is R Code\"\n",
    "Two: Put your mouse in the cell anywhere and hit the Run button at the top of the browser.\n",
    "Three: Put your mouse in the cell anywhere and press Ctrl-Enter (all at once) Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is R code\n",
    "#When I have text like this with a # at the front, it makes it a \"comment\"\n",
    "#This means that it never does anything. It's just explanatory text.\n",
    "#Comments are helpful so you can explain to yourself or others what you're doing.\n",
    "\n",
    "#I can execute code by typing something without a #\n",
    "\n",
    "2+2\n",
    "\n",
    "#If I hit the run button, it will run this entire set.\n",
    "#The only line that does anything, though is 2+2. The answer will appear below.\n",
    "\n",
    "#Hit Run now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_example <- \"hello world\"\n",
    "text_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have text again. In the code cells above, we first told R to calculate 2+2. We then told R to say \"hello world.\" The way I did that was I took what I wanted to say - hello world - and put it in quotes. I then took that text and assigned it to a variable called 'text_example' using the arrow. text_example is a random name I made up. It could have been anything. Usually, you want to make up names that help you remember what you're doing. Finally, by saying |text_example|, I told R to run that text_example. This printed it out on the screen.\n",
    "\n",
    "The main thing to learn from this is that you can write a bunch of instructions and assign them to something like text_example to run. To get a handle on this, we'll do a few more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_example_2 <- \"Chihuahuas are yappy\"\n",
    "text_random <- \"Hi, I hope you're following along fine\"\n",
    "something <- \"gobbley gook\"\n",
    "text_example_2\n",
    "text_random\n",
    "something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't very exciting yet, since we are just doing 2+2 and printing things I type in. But, of course, the instructions can be a lot more interesting, such as downloading texts from the web automatically, creating detailed graphs, doing complex analysis, running simulations, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise this introduction section, we've made R do something by running lines of code. Moreover, we're doing it inside a Jupyter notebook, which includes explanatory text, code to execute, and the output of that code all in one place. Without a notebook like this, you would have R itself, which is all code and output, but no explanation. And a textbook is explanation, but doesn't do anything. This Jupyter notebook puts it all together.\n",
    "\n",
    "Why \"Jupyter\"? It stands for Julia, Python, and R, which are three very common programming languages. We'll be using mostly R and Python in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the System for Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to prepare everything for analysis. There are two main things we need to do.\n",
    "\n",
    "First, we will activate all of the software we need to analysing texts.\n",
    "Second, we will download texts to examine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R was originally designed for statistical analysis. When you first install it, it has built-in functionality for that. For example, it can do a regression analysis or correlations. It can also do some basic plotting. This is great and it's why I got into R in the first place. However, R is an open source project, and now many, many developers are creating R Packages, which are extra bits of software that extends what R can do. We will be using some packages that - unsurprisingly! - are designed for text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library function tells R to use the named package for this session.\n",
    "\n",
    "tidyverse is a very large but important package that allows for a whole range of functions. Run this next line to load it. It will take a couple minutes and will install a whole series of packages. How do you know when it's done? When a line of code is executing, it has an asterisk. When it finishes, a number appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the package \"tidyverse\"\n",
    "install.packages(\"tidyverse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will install additional packages. These include:\n",
    "\n",
    "tidytext <-- for analysing text in the \"tidy\" format\n",
    "gutenbergr <-- downloads and prepares texts from gutenberg\n",
    "janeaustenr <-- downloads Jane Austen novels!\n",
    "wordcloud <-- build word clouds directly in R\n",
    "topicmodels <-- used for identifying the topics of a given text\n",
    "rtweet <-- used to download twitter data\n",
    "\n",
    "We won't have time to explore all of these, but we're prepared in case we run fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to install all the packages\n",
    "\n",
    "#I have commented the code out because it should be installed in the lab environment.\n",
    "#But if you are home, you would need to run these lines.\n",
    "#To do so, remove the # before the install instructions.\n",
    "#install.packages(\"tidytext\")\n",
    "#install.packages(\"gutenbergr\")\n",
    "#install.packages(\"janeaustenr\")\n",
    "#install.packages(\"wordcloud\")\n",
    "#install.packages(\"reshape2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The install instruction simply puts the packages on your computer. To actually use them in the session, you use the library command. This may give you warning messages about version numbers. That's fine. You can ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell\n",
    "\n",
    "library(dplyr)\n",
    "library(scales)\n",
    "library(gutenbergr)\n",
    "library(janeaustenr)\n",
    "library(ggplot2)\n",
    "library(stringr)\n",
    "library(tidyr)\n",
    "library(tidytext)\n",
    "library(wordcloud)\n",
    "library(reshape2)\n",
    "library(forcats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not need to install packages every time. When a package is installed, it's installed. However, you will need to run the library command anytime you come back to this. This is not an issue for the lab, but, when working on your project over the semester, you may need to run the library command again. You will know this is the case if R tells you it cannot find something that used to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the software we need. The next step is download some text to analyse. We will use the gutenberg corpus of texts and the gutenbergr package.\n",
    "\n",
    "Every book in Project Gutenberg has an ID. We need to know that ID so we can tell R which book to get for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the metadata for works, filter by title\n",
    "gutenberg_metadata %>%\n",
    "    filter(title == \"Jane Eyre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small table will appear listing the books available. The gutenberg_id for Jane Eyre is 23077. If you wanted to get all the books by an author, you could search with an author filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the metadata for works, filter by author\n",
    "gutenberg_metadata %>%\n",
    "    filter(author == \"Brontë, Charlotte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This now lists all the books by that author. Please note that what is inside the filter must match EXACTLY. If the comma is missing in \"Brontë, Charlotte\", it will find nothing. If the space is missing, it will find nothing. If the ë is a regular e, it finds nothing. If Charlotte is not capitalised, it finds nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, let's download the novel Wuthering Heights. We need its number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the metadata for works, filter by title\n",
    "gutenberg_metadata %>%\n",
    "    filter(title == \"Wuthering Heights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wuthering Heights is id 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line says download the book 768 and put it into the variable named 'w_heights'\n",
    "w_heights <- gutenberg_download(768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running this next line will print the entire book. (It will cut off after some lines.)\n",
    "w_heights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: In this section, we have prepared R for all the analysis that we want to do. Plus, we have downloaded a book from Project Gutenberg.\n",
    "\n",
    "Now, we are ready to analyse it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine and Visualise Word Frequency in Wuthering Heights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to actually analyse some text! We will start by looking at the frequency of words. When you ran the w_heights line of code above, you will have seen that the book was downloaded in a book format with headings, sentences, paragraphs, etc. To count the words, we have to change that format so that each word is its own item. For example, we will turn \"each word is its own item\" into a list:\n",
    "\n",
    "each\n",
    "word\n",
    "is\n",
    "its\n",
    "own\n",
    "item\n",
    "\n",
    "We also have to deal with details like removing punctuation and ignoring capital letter. This process is called tokenisation. If we were doing this by hand, we would perhaps create a notebook with page after page where each line is a word. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the downloaded book, turn it into tokens, and put it in an item called tidy_W_heights\n",
    "tidy_W_heights <- w_heights %>%\n",
    "  unnest_tokens(word,text)\n",
    "\n",
    "#now we will count up all of these word tokens using the count function\n",
    "twh_count<-tidy_W_heights %>%\n",
    "  count(word,sort = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that many of these commands have a symbol: %>% This is called \"pipe\". It roughly means \"and then\", so that you take what you did in the last step and then do something else. You can use pipe (%>%) to put several commands in a row.\n",
    "\n",
    "If we wanted to send this count result out to Excel to see a complete list, we could do it with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a csv file called \"wh_word_freq.csv\" that contains the results in twh_count\n",
    "\n",
    "write.csv(twh_count,file = \"wh_word_freq.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you create a file like twh_count, or anything, it stays in memory for you to use later. The last line tells R to write out a file that we created a couple steps ago. In fact, way at the top we created a little thing called |text_example|. That's still there. If you run it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still there. All the files you create in this lab session remain through the session to use. However, if you stop this session, you must start from the top again. This is not the case in using R at home where you can save the \"workspace\" which keeps everything there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've introduced the concept of Zipf's law or power laws which have a few items that are large in size and many, many items that are small. We could now test if that is true for the words of Wuthering Heights. We will create two plots, one which just has the word count for every word in the book, and another which does the same thing but puts it on a logarithmic scale. From the video, we would expect the words to be close to a straight line on the logarithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we graph the raw data points. The x-axis will just be the number of words.\n",
    "#The y-axis is the count.\n",
    "\n",
    "#Use the ggplot software to calculate the plot.\n",
    "gg.1 <- ggplot(data = twh_count,aes(x=1:length(twh_count$n),y=n))+geom_point()\n",
    "gg.2 <- gg.1+labs(x=\"Words\",y=\"Word Count\")\n",
    "print(gg.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph was calculated in two steps. First I created the graph, saying what was on the x-axis and y-axis. I called this gg.1. I then added some labels to gg.1, calling that result gg.2. Finally, I printed gg.2 to see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now convert this to a log-log format to see how straight the line is\n",
    "\n",
    "gg.3 <- ggplot(data=twh_count,aes(x=1:length(twh_count$n),y=n))+geom_point()+geom_smooth(method=lm)+\n",
    "  scale_x_log10(labels=percent_format())+scale_y_log10(labels=percent_format())\n",
    "gg.4 <- gg.3+labs(x=\"Words\",y=\"Word Count\")\n",
    "print(gg.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot is approximately straight but veers from the line some for the high frequency words. This is common with a corpus of words that is big but not very big. A single book looks somewhat like a power law, but we would expect that 20, 100, or 1000 books would look more and more like a power law.\n",
    "\n",
    "There can be reasons of interest at times, however, that a single book doesn't follow a power law exactly. This often reflects the topics of the book such that some words are used abnormally frequently, because that's what the book is about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should also be clear at this point that I'm not walking through all of the syntax of the code. I felt that doing so would bog things down too much. By the time we learned all of the nuances of the syntax, we wouldn't have any mental space left for the actual text analysis. I will fill in gaps as people ask questions and we discover the need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have calculated word frequency and tested it against the power law prediction. Let's think about visualisation next. The wordcloud package can create a word cloud, so let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the count file we created and send it to a word cloud with a max of 100 words.\n",
    "twh_count %>%\n",
    "  with(wordcloud(word,n,max.words = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly see that the most common words are function words like \"the\" and \"and\". If that's what we're interested in, then great! But often we want to look at the content words more. These can be removed. There is a common list of function and extremely common words already created. This list is called \"stop words\". We can create the word cloud again with stop words removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this line is the same as above but we remove stop words before creating the cloud.\n",
    "twh_count %>%\n",
    "  anti_join(stop_words) %>%\n",
    "  with(wordcloud(word,n,max.words = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been looking at all of the words in the text. However, we might want to filter those words in some interesting way. For example, if we are interested in emotions within the books, we could look at just positive or just negative emotion words. This is called sentiment analysis.\n",
    "\n",
    "How do you know what words are positive and negative sentiments?\n",
    "\n",
    "Researchers have created databases labelling many words in such a way. The tidytext package includes three such databases. One is called \"NRC\" and includes words for joy, sadness, fear, etc. What we will do next is filter so only certain words are still represented.\n",
    "\n",
    "Call the NRC sentiment list and look at joy. The software will ask you to cite this data if published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of joy word from the nrc list, called nrc_joy\n",
    "nrc_joy <- get_sentiments(\"nrc\") %>% \n",
    "  filter(sentiment == \"joy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take all the words in Wuthering Heights and give us the common joy ones\n",
    "tidy_W_heights %>%\n",
    "  inner_join(nrc_joy) %>%\n",
    "  count(word, sort = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We could do the same thing but send to a wordcloud. I write the same lines as above, but pipe it to a word cloud.\n",
    "tidy_W_heights %>%\n",
    "  inner_join(nrc_joy) %>%\n",
    "  count(word, sort = TRUE) %>%\n",
    "  with(wordcloud(word,n,max.words = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nrc_joy is just a list of joyous words to filter by. If you wanted to filter by something else, the same technique can be used. To discover what other sentiments are present in the NRC database, we would get the whole database and not filter for joy. What sentiments exist in it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the sentiments\n",
    "nrc <- get_sentiments(\"nrc\")\n",
    "#Now ask for all the unique sentiments.\n",
    "unique(nrc$sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might then do the same task as above but with sadness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the sad word list\n",
    "nrc_sadness <- get_sentiments(\"nrc\") %>% \n",
    "  filter(sentiment == \"sadness\")\n",
    "#create a sad word cloud for wuthering heights\n",
    "tidy_W_heights %>%\n",
    "  inner_join(nrc_sadness) %>%\n",
    "  count(word, sort = TRUE) %>%\n",
    "  with(wordcloud(word,n,max.words = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRACTICTE TIME!!\n",
    "\n",
    "Choose another emotion in the nrc list and get the word cloud for that emotion.\n",
    "\n",
    "To do that, copy the code above into the next cell. Change the sadness emotion to another and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put your code here. If there is a technical problem doing this, just make a note on what you would do and run it by me.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#code above. If this works, you have earned your lab point for the day.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we will want to compare texts. We could look at common words for multiple texts. Or do sentiment analysis for multiple texts. Or look for what words are unusual compared to other texts.\n",
    "\n",
    "Let's look at other Bronte sister books.\n",
    "\n",
    "Download Charlotte and Anne Bronte novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_metadata %>%\n",
    "  filter(author == \"Brontë, Charlotte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eyre <- gutenberg_download(1260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_metadata %>%\n",
    "  filter(author == \"Brontë, Anne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenant <- gutenberg_download(969)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure it's clear what the lines above are doing. These lines repeat the ideas from above but with new books. If it's not clear, grab me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by tokenising and generating word clouds for each book: Wuthering Heights, Jane Eyre, and The Tenant of Wildfell Hall.\n",
    "\n",
    "What is \"tokenising\"? Ask if not sure.\n",
    "\n",
    "I will be repeating some steps from above so that we can see everything together. Frequency counts are quite fast, so it's easy to just repeat things. Also, I will remove stop_words as uninteresting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a word cloud for Wuthering Heights\n",
    "w_heights %>%\n",
    "  unnest_tokens(word,text) %>%\n",
    "  anti_join(stop_words) %>%\n",
    "  count(word,sort = TRUE) %>%\n",
    "  with(wordcloud(word,n,max.words = 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a word cloud for Jane Eyre\n",
    "eyre %>%\n",
    "  unnest_tokens(word,text) %>%\n",
    "  anti_join(stop_words) %>%\n",
    "  count(word,sort = TRUE) %>%\n",
    "  with(wordcloud(word,n,max.words = 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a word cloud for The Tenant of Wildfell Hall\n",
    "tenant %>%\n",
    "  unnest_tokens(word,text) %>%\n",
    "  anti_join(stop_words) %>%\n",
    "  count(word,sort = TRUE) %>%\n",
    "  with(wordcloud(word,n,max.words = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment as you create these to look at the results and see what it reveals about the books. Also, remember these are all examples.\n",
    "\n",
    "You could do this with other individual novels. You could do it with whole collections of novels. For instance, what are commone words in different time periods or genres. You could do this with any text, be it literature, history, physics, sociology, etc. It could be novels, poetry, newspaper articles, journal articles, etc. This is digital humanities. We're just using literature as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now do a comparative sentiment analysis for all three books. We will do the same thing as above but filter on the joyful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment word cloud for Wuthering Heights\n",
    "w_heights %>%\n",
    "  unnest_tokens(word,text) %>%\n",
    "  anti_join(stop_words) %>%\n",
    "  inner_join(nrc_joy) %>%\n",
    "  count(word, sort = TRUE) %>%\n",
    "  with(wordcloud(word,n,max.words = 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment word cloud for Jane Eyre\n",
    "eyre %>%\n",
    "  unnest_tokens(word,text) %>%\n",
    "  anti_join(stop_words) %>%\n",
    "  inner_join(nrc_joy) %>%\n",
    "  count(word, sort = TRUE) %>%\n",
    "  with(wordcloud(word,n,max.words = 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analysis for The Tenant of Wildfell Hall\n",
    "tenant %>%\n",
    "  unnest_tokens(word,text) %>%\n",
    "  anti_join(stop_words) %>%\n",
    "  inner_join(nrc_joy) %>%\n",
    "  count(word, sort = TRUE) %>%\n",
    "  with(wordcloud(word,n,max.words = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word clouds are somewhat impressionistic. Another comparison we can make is more systematic. This method uses a measure called tf-idf, which stands for text frequence - inverse document frequency. This measure looks at what words are common across a corpus. It then looks for words in a specific text that are uncommon in that corpus.\n",
    "\n",
    "For instance, the word \"the\" is frequent in every book and so not notable. Similarly, if all books in your corpus frequently mention \"love\" it would not be notable. However, if one text uses \"love\" a lot and others do not, then \"love\" distinguishes that book. This is what we'll look for next.\n",
    "\n",
    "We want to compare several books at once, so we must prepare the data. Our goal is to create a single file which has all of the books together so that the tf-idf algorithm can compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we get a count for each book\n",
    "twh_count <- w_heights %>%\n",
    "  unnest_tokens(word,text) %>%\n",
    "  count(word, sort = TRUE)\n",
    "  \n",
    "\n",
    "te_count <- eyre %>%\n",
    "  unnest_tokens(word,text) %>%\n",
    "  count(word, sort = TRUE)\n",
    "\n",
    "tt_count <- tenant %>%\n",
    "  unnest_tokens(word,text) %>%\n",
    "  count(word, sort = TRUE)\n",
    "\n",
    "#Notice that all of the code here is the same as before. We just gave it a name.\n",
    "\n",
    "#next we add a column to each count to recordi what book it is\n",
    "#we're doing this so that when we put them all together we still know what book the word is from.\n",
    "\n",
    "twh_count$book <- \"Heights\"\n",
    "te_count$book <- \"eyre\"\n",
    "tt_count$book <- \"tenant\"\n",
    "\n",
    "#next we put them together\n",
    "bronte_count <- rbind(twh_count,te_count,tt_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of that block of code is a file called |bronte_count| which contains all three book.\n",
    "\n",
    "Now we're ready to compare documents to look for what's uncommon in the books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the bronte_count file and run the tf-idf algorithm on it. Put it in a file called bronte_tf_idf\n",
    "bronte_tf_idf <- bronte_count %>%\n",
    "  bind_tf_idf(word,book,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculated the tf-idf value for all the words. We can focus on the unusual ones with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronte_tf_idf %>%\n",
    "  arrange(desc(tf_idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: In this entire lab, we've learned about the R and Jupyter notebook format. We then downloaded texts to analyse. We calculated frequency of words and then visualised them as an x-y plot and a word cloud. We next learned sentiment analysis and how to compare texts. We finished with the tf-idf measure comparing texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal now is to do this sort of analysis with a new book. You should be able to copy the code above and modify it slightly to get a result. Again, if you can't run modified code, look above and make notes on how you would do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a book from Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenise that text and create a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a sentiment and generate a word cloud of that sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You're done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
